{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:07.399494Z","iopub.status.busy":"2024-04-05T20:52:07.399012Z","iopub.status.idle":"2024-04-05T20:52:12.265675Z","shell.execute_reply":"2024-04-05T20:52:12.264462Z","shell.execute_reply.started":"2024-04-05T20:52:07.399463Z"},"trusted":true},"outputs":[],"source":["import networkx as nx\n","import numpy as np\n","import pickle as p\n","from os import path\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import torch\n","data_loc = './data/BlogCatalog3/BlogCatalog-dataset/data/'\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:12.267820Z","iopub.status.busy":"2024-04-05T20:52:12.267311Z","iopub.status.idle":"2024-04-05T20:52:12.301396Z","shell.execute_reply":"2024-04-05T20:52:12.300086Z","shell.execute_reply.started":"2024-04-05T20:52:12.267790Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import random\n","\n","def gen_random_walk_tensor(graph, node, length, num_walks):\n","    walk = torch.zeros((num_walks, length), dtype=int)\n","    walk[:, 0] = node\n","    j = 0\n","    while j < num_walks:\n","        current_node = node\n","        step = 1\n","        while step < length:\n","            neighbors = list(graph.neighbors(current_node))\n","            current_node = random.choice(neighbors)\n","            walk[j, step] = current_node\n","            step += 1\n","        j+=1\n","    return walk\n","\n","def gen_batch_random_walk(graph, initial_nodes, length, num_walks):\n","    n_nodes = initial_nodes.shape[0]\n","    walk = torch.zeros((num_walks*n_nodes, length), dtype=int)\n","    for i, n in enumerate(initial_nodes):\n","        n = n.item()\n","        walk[num_walks*i:num_walks*(i+1)] = gen_random_walk_tensor(graph, n, length, num_walks)\n","    return walk\n","\n","def generate_windows(random_walk, window_size):\n","    num_walks, walk_length = random_walk.shape\n","    # number of windows: e.g. length 5, window size 3 -> 3 windows ([0, 1, 2], [1, 2, 3], [2, 3, 4])\n","    num_windows = walk_length + 1 - window_size\n","    windows = torch.zeros((num_walks*num_windows, window_size), dtype=int)\n","    for j in range(num_windows):\n","        windows[num_walks*j:num_walks*(j+1)] = random_walk[:, j:j+window_size]\n","    return windows\n","\n","def get_windows_dotproduct(windows, embedding):\n","    embedding_size = embedding.shape[1]\n","    # get the embedding of the initial node repeated num_windows times\n","    first_emb = embedding[windows[:, 0]]\n","    first_emb = first_emb.view(windows.shape[0], 1, embedding_size)\n","    # get the embedding of the remaining nodes in each window\n","    others_emb = embedding[windows[:, 1:]]\n","    others_emb = others_emb.view(windows.shape[0], -1, embedding_size)\n","    # result has same shape as others\n","    # Each element is the dot product between the corresponding node embedding\n","    # and the embedding of the first node of that walk\n","    # that is, result_{i, j} for random walk i and element j is v_{W_{i, 0}} dot v_{W_{i, j}}\n","    result = (first_emb*others_emb).sum(dim=-1)\n","    return result\n","\n","def gen_negative_samples(amount, length, initial_node, number_of_nodes):\n","    negative_samples = torch.zeros((amount, length), dtype=int)\n","    negative_samples[:, 0] = initial_node\n","    negative_samples[:, 1:] = torch.randint(number_of_nodes, (amount, length-1))\n","    return negative_samples\n","\n","def gen_batch_negative_samples(amount, length, initial_nodes, number_of_nodes, distribution=None):\n","    negative_samples = torch.zeros((amount*initial_nodes.shape[0], length), dtype=int)\n","    negative_samples[:, 0] = initial_nodes.repeat(amount, 1).t().contiguous().view(-1)\n","    if distribution is None:\n","        negative_samples[:, 1:] = torch.randint(number_of_nodes, (amount*initial_nodes.shape[0], length-1))\n","    else:\n","        M = amount*initial_nodes.shape[0]\n","        N = length-1\n","        negative_samples[:, 1:] = torch.multinomial(distribution, M*N, replacement=True).view(M, N)\n","    return negative_samples\n","\n","def gen_biaised_random_walk_tensor(graph, start_node, walk_length, num_walks, p, q , neighbors_dict):\n","    walks = torch.zeros((num_walks, walk_length), dtype=int)\n","    walks[:, 0] = start_node\n","\n","    for walk_index in range(num_walks):\n","        current_node = start_node\n","        for step in range(walk_length):\n","            walks[walk_index, step] = current_node\n","            if step > 0:\n","                prev_node = int(walks[walk_index, step - 1])\n","                current_node = get_next_node(graph,prev_node,current_node,p,q)\n","            else:\n","                current_node = np.random.choice(list(graph.neighbors(current_node)))\n","            \n","    \n","    return walks\n","\n","def get_next_node(graph,t,v, p, q):\n","    v_neighbors = set(graph.neighbors(v))\n","    t_neighbors = set(graph.neighbors(t))\n","    t_set = set([t])\n","\n","    vt_neighbors = v_neighbors & t_neighbors\n","    only_v_neighbors = v_neighbors - t_neighbors - t_set\n","\n","    allsets = [vt_neighbors,only_v_neighbors,t_set]\n","\n","    vt_weights = 1 * len(vt_neighbors)\n","    only_v_weights = 1/q * len(only_v_neighbors)\n","    t_weight = 1/p\n","\n","    prob_vector = np.array((vt_weights,only_v_weights,t_weight))\n","    prob_vector = prob_vector / np.sum(prob_vector)\n","\n","    chosen_set = np.random.choice(allsets,p=prob_vector)\n","    next_node = np.random.choice(list(chosen_set))\n","    return next_node\n","\n","def gen_batch_biaised_random_walk(graph, initial_nodes, length, num_walks, p, q, neighbors_dict):\n","    n_nodes = initial_nodes.shape[0]\n","    walk = torch.zeros((num_walks*n_nodes, length), dtype=int)\n","    for i, n in enumerate(initial_nodes):\n","        n = n.item()\n","        walk[num_walks*i:num_walks*(i+1)] = gen_biaised_random_walk_tensor(graph, n, length, num_walks, p, q,neighbors_dict)\n","    return walk"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:12.304061Z","iopub.status.busy":"2024-04-05T20:52:12.303628Z","iopub.status.idle":"2024-04-05T20:52:12.323672Z","shell.execute_reply":"2024-04-05T20:52:12.322321Z","shell.execute_reply.started":"2024-04-05T20:52:12.304023Z"},"trusted":true},"outputs":[],"source":["import psutil"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:12.327175Z","iopub.status.busy":"2024-04-05T20:52:12.326798Z","iopub.status.idle":"2024-04-05T20:52:12.565324Z","shell.execute_reply":"2024-04-05T20:52:12.562567Z","shell.execute_reply.started":"2024-04-05T20:52:12.327133Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","import numpy as np\n","import random\n","\n","eps = 1e-15\n","\n","def generate_batches(array, batch_size):\n","    \"\"\"Yield successive batches of size `batch_size` from `array`.\"\"\"\n","    for i in range(0, len(array), batch_size):\n","        yield array[i:i + batch_size]\n","\n","def deepWalk(graph, walks_per_vertex, walk_length, window_size, embedding_size, num_neg, lr, epochs, batch_size, distribution=None):\n","    number_of_nodes = graph.number_of_nodes()\n","    \n","    embedding = (torch.randn(size=(number_of_nodes, embedding_size))).detach()\n","    embedding.requires_grad = True\n","    optimizer = torch.optim.Adam([embedding], lr=lr)\n","    loss_history = {'pos': [], 'neg': [], 'total': []}\n","    nodes = torch.tensor(list(graph.nodes), dtype=int)\n","\n","    for _ in range(epochs):\n","        random_ixs = torch.randperm(nodes.shape[0])\n","        nodes = nodes[random_ixs]\n","        node_loader = generate_batches(nodes, batch_size)\n","        n_batches = int(number_of_nodes / batch_size)\n","        for n in tqdm(node_loader, total=n_batches):\n","\n","            random_walk = gen_batch_random_walk(graph, n, walk_length, walks_per_vertex)\n","            # Positive Sampling\n","            # each row of windows is one window, we have B = walks_per_vertex*num_windows windows\n","            windows = generate_windows(random_walk, window_size)\n","            batch_dotproduct = get_windows_dotproduct(windows, embedding)\n","            # takes the sigmoid of the dot product to get probability, then\n","            # takes the loglik and average through all elements\n","            pos_loss = -torch.log(torch.sigmoid(batch_dotproduct)+eps).mean()\n","            # Negative Sampling\n","            negative_samples = gen_batch_negative_samples(\n","                amount=num_neg*walks_per_vertex, \n","                length=walk_length, \n","                initial_nodes=n, \n","                number_of_nodes=number_of_nodes,\n","                distribution=distribution\n","            )\n","            windows = generate_windows(negative_samples, window_size)\n","            batch_dotproduct = get_windows_dotproduct(windows, embedding)\n","            neg_loss = -torch.log(1-torch.sigmoid(batch_dotproduct)+eps).mean()\n","\n","            loss = pos_loss + neg_loss\n","            # Optimization\n","            loss.backward()\n","            loss_history['total'].append(loss.detach().numpy())\n","            loss_history['pos'].append(pos_loss.detach().numpy())\n","            loss_history['neg'].append(neg_loss.detach().numpy())\n","            optimizer.step()\n","            optimizer.zero_grad()  \n","\n","    return embedding, loss_history\n","\n","def node2vec(graph, walks_per_vertex, walk_length, window_size, embedding_size, num_neg, lr, epochs, batch_size, p = 5, q = 5):\n","    number_of_nodes = graph.number_of_nodes()\n","    \n","    embedding = (torch.randn(size=(number_of_nodes, embedding_size)) ).detach()\n","    embedding.requires_grad = True\n","    optimizer = torch.optim.Adam([embedding], lr=lr)\n","    loss_history = {'pos': [], 'neg': [], 'total': []}\n","    neighbors_dict = {node: list(graph.neighbors(node)) for node in graph.nodes}\n","    nodes = torch.tensor(list(graph.nodes), dtype=int)\n","\n","    for _ in range(epochs):\n","        random.shuffle(nodes)\n","        node_loader = generate_batches(nodes, batch_size)\n","        n_batches = int(number_of_nodes / batch_size)\n","        for n in tqdm(node_loader, total=n_batches):\n","            random_walk = gen_batch_biaised_random_walk(graph, n, walk_length, walks_per_vertex, p, q, neighbors_dict)\n","            num_windows = walk_length + 1 - window_size\n","\n","            # Positive Sampling\n","            # each row of windows is one window, we have B = walks_per_vertex*num_windows windows\n","            windows = generate_windows(random_walk, window_size)\n","            batch_dotproduct = get_windows_dotproduct(windows, embedding)\n","            # takes the sigmoid of the dot product to get probability, then\n","            # takes the loglik and average through all elements\n","            pos_loss = -torch.log(torch.sigmoid(batch_dotproduct)+eps).mean()\n","            # Negative Sampling\n","            negative_samples = gen_batch_negative_samples(\n","                amount=num_neg*walks_per_vertex, \n","                length=walk_length, \n","                initial_nodes=n, \n","                number_of_nodes=number_of_nodes\n","            )\n","            windows = generate_windows(negative_samples, window_size)\n","            batch_dotproduct = get_windows_dotproduct(windows, embedding)\n","            neg_loss = -torch.log(1-torch.sigmoid(batch_dotproduct)+eps).mean()\n","\n","            loss = pos_loss + neg_loss\n","            # Optimization\n","            loss.backward()\n","            loss_history['total'].append(loss.detach().numpy())\n","            loss_history['pos'].append(pos_loss.detach().numpy())\n","            loss_history['neg'].append(neg_loss.detach().numpy())\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    return embedding, loss_history\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:12.567920Z","iopub.status.busy":"2024-04-05T20:52:12.567265Z","iopub.status.idle":"2024-04-05T20:52:14.896640Z","shell.execute_reply":"2024-04-05T20:52:14.895455Z","shell.execute_reply.started":"2024-04-05T20:52:12.567886Z"},"trusted":true},"outputs":[],"source":["def load_data():\n","    iid = {}\n","    idx = 0\n","    edgelist = []\n","\n","    # Read edges pairs\n","    with open(data_loc+'edges.csv', 'r') as f:\n","        for line in f.readlines():\n","            i, j = line.strip().split(',')  # csv\n","            if i not in iid:\n","                iid[i] = idx; idx += 1\n","            if j not in iid:\n","                iid[j] = idx; idx += 1\n","            edgelist.append((iid[i], iid[j]))\n","\n","    # Create an nx undirected network\n","    bc = nx.Graph(edgelist)\n","\n","    print(\"Number of nodes: \", len(bc))\n","    print(\"Number of edges: \", bc.size())\n","\n","    # Read labels\n","    labels = np.zeros((len(bc)), dtype=int)\n","    # Read (node_id, label) file\n","    with open(data_loc+'group-edges.csv', 'r') as f:\n","        for line in f.readlines():\n","            node, group = line.strip().split(',') \n","            labels[iid[node]] = int(group)-1  \n","\n","    bc_dataset = {'graph': bc, 'labels': labels}\n","    return bc_dataset\n","\n","bc_dataset = load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:14.898623Z","iopub.status.busy":"2024-04-05T20:52:14.898044Z","iopub.status.idle":"2024-04-05T20:52:14.905932Z","shell.execute_reply":"2024-04-05T20:52:14.904498Z","shell.execute_reply.started":"2024-04-05T20:52:14.898592Z"},"trusted":true},"outputs":[],"source":["import logging\n","logging.basicConfig(filename='./deepWalk_bayesian_opt.log', filemode='w',level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logging.info(\"Starting optimization\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:14.911633Z","iopub.status.busy":"2024-04-05T20:52:14.910097Z","iopub.status.idle":"2024-04-05T20:52:16.711547Z","shell.execute_reply":"2024-04-05T20:52:16.710338Z","shell.execute_reply.started":"2024-04-05T20:52:14.911587Z"},"trusted":true},"outputs":[],"source":["from skopt.space import Real, Categorical, Integer\n","from skopt import gp_minimize\n","from skopt.utils import use_named_args\n","from IPython.display import clear_output\n","\n","walks_per_vertex = 1\n","embedding_size = 128\n","window_size = 10\n","walk_length = 80\n","epochs = 10\n","batch_size = 64\n","\n","dim_num_neg = Integer(1,10,name=\"num_neg\")\n","dim_lr = Real(\n","    low=1e-3, high=1e-1, prior='log-uniform', name='lr',\n",")\n","\n","\n","# the hyperparameter space grid\n","param_grid = [dim_num_neg,dim_lr]\n","\n","\n","@use_named_args(param_grid)\n","def objective(num_neg,lr):    \n","    \n","    clear_output(wait=True)\n","    print(\"Current Training\")\n","    print(f\"    walks_per_vertex: {walks_per_vertex}\")\n","    print(f\"    walk_length: {walk_length}\")\n","    print(f\"    window_size: {window_size}\")\n","    print(f\"    embedding_size: {embedding_size}\")\n","    print(f\"    num_neg: {num_neg}\")\n","    print(f\"    lr: {lr}\")\n","    print(f\"    epochs: {epochs}\")\n","    print(f\"    batch_size: {batch_size}\")\n","\n","    logging.info(\"Starting optimization with following parameters:\")\n","    logging.info(f\"     walks_per_vertex: {walks_per_vertex}\")\n","    logging.info(f\"     walk_length: {walk_length}\")\n","    logging.info(f\"     window_size: {window_size}\")\n","    logging.info(f\"     embedding_size: {embedding_size}\")\n","    logging.info(f\"     num_neg: {num_neg}\")\n","    logging.info(f\"     lr: {lr}\")\n","    logging.info(f\"     epochs: {epochs}\")\n","    logging.info(f\"     batch_size: {batch_size}\")\n","\n","\n","\n","\n","\n","    embedding, loss_history = deepWalk(graph=bc_dataset['graph'],\n","                                walks_per_vertex=walks_per_vertex,\n","                                walk_length=walk_length,\n","                                window_size=window_size,\n","                                embedding_size=embedding_size,\n","                                num_neg=num_neg,\n","                                lr=lr,\n","                                epochs=epochs,\n","                                batch_size=batch_size)\n","    logging.info(f\"Final loss: {loss_history['total'][-1]}\")\n","    return float(loss_history['total'][-1])\n","\n","\n","default_parameters = [3,1e-2]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T20:52:16.713744Z","iopub.status.busy":"2024-04-05T20:52:16.713368Z"},"trusted":true},"outputs":[],"source":["gp_ = gp_minimize(\n","    objective, # the objective function to minimize\n","    param_grid, # the hyperparameter space\n","    x0=default_parameters, # the initial parameters to test\n","    acq_func='EI', # the acquisition function\n","    n_calls=20, # the number of subsequent evaluations of f(x)\n","    random_state=0,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"Best score=%.4f\" % gp_.fun"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gp_.x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open('./output/Deepwalk_gp.pickle', 'wb') as f:\n","    pickle.dump(gp_, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_neg,lr = gp_.x\n","\n","embedding, loss_history = deepWalk(graph=bc_dataset['graph'],\n","                                walks_per_vertex=walks_per_vertex,\n","                                walk_length=walk_length,\n","                                window_size=window_size,\n","                                embedding_size=embedding_size,\n","                                num_neg=num_neg,\n","                                lr=lr,\n","                                epochs=epochs,\n","                                batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","from sklearn.linear_model import LogisticRegression\n","\n","X = embedding.detach().numpy()\n","y = bc_dataset['labels']\n","\n","clf = LogisticRegression(random_state=0,max_iter=1000).fit(X, y)\n","y_hat = clf.predict(X)\n","f1_score(y, y_hat, average='macro')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4662205,"sourceId":8036262,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
