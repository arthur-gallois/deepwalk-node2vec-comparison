{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e539cf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepmind/jraph.git\n",
      "  Cloning https://github.com/deepmind/jraph.git to c:\\users\\alex_\\appdata\\local\\temp\\pip-req-build-j6sv_55i\n",
      "  Resolved https://github.com/deepmind/jraph.git to commit 51f5990104f7374492f8f3ea1cbc47feb411c69c\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jax>=0.1.55 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jraph==0.0.6.dev0) (0.4.23)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jraph==0.0.6.dev0) (0.4.23)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jraph==0.0.6.dev0) (1.26.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.1.55->jraph==0.0.6.dev0) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.1.55->jraph==0.0.6.dev0) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.1.55->jraph==0.0.6.dev0) (1.11.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/deepmind/jraph.git 'C:\\Users\\alex_\\AppData\\Local\\Temp\\pip-req-build-j6sv_55i'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flax in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (1.26.2)\n",
      "Requirement already satisfied: jax>=0.4.19 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (0.4.23)\n",
      "Requirement already satisfied: msgpack in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (1.0.7)\n",
      "Requirement already satisfied: optax in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (0.1.8)\n",
      "Requirement already satisfied: orbax-checkpoint in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (0.5.1)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (0.1.52)\n",
      "Requirement already satisfied: rich>=11.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (13.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax) (6.0.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.4.19->flax) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.4.19->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.4.19->flax) (1.11.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alex_\\appdata\\roaming\\python\\python311\\site-packages (from rich>=11.1->flax) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from optax->flax) (2.1.0)\n",
      "Requirement already satisfied: chex>=0.1.7 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from optax->flax) (0.1.85)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from optax->flax) (0.4.23)\n",
      "Requirement already satisfied: etils[epath,epy] in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\alex_\\appdata\\roaming\\python\\python311\\site-packages (from orbax-checkpoint->flax) (1.5.6)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from orbax-checkpoint->flax) (4.23.4)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from chex>=0.1.7->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2023.12.2)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\n",
      "Requirement already satisfied: zipp in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n",
      "Requirement already satisfied: dm-haiku in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (0.0.12)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from dm-haiku) (2.1.0)\n",
      "Requirement already satisfied: jmp>=0.0.2 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from dm-haiku) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from dm-haiku) (1.26.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from dm-haiku) (0.9.0)\n",
      "Requirement already satisfied: flax>=0.7.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from dm-haiku) (0.7.5)\n",
      "Requirement already satisfied: jax>=0.4.19 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (0.4.23)\n",
      "Requirement already satisfied: msgpack in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (1.0.7)\n",
      "Requirement already satisfied: optax in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (0.1.8)\n",
      "Requirement already satisfied: orbax-checkpoint in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (0.5.1)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (0.1.52)\n",
      "Requirement already satisfied: rich>=11.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (13.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from flax>=0.7.1->dm-haiku) (6.0.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.4.19->flax>=0.7.1->dm-haiku) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.4.19->flax>=0.7.1->dm-haiku) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from jax>=0.4.19->flax>=0.7.1->dm-haiku) (1.11.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from rich>=11.1->flax>=0.7.1->dm-haiku) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alex_\\appdata\\roaming\\python\\python311\\site-packages (from rich>=11.1->flax>=0.7.1->dm-haiku) (2.14.0)\n",
      "Requirement already satisfied: chex>=0.1.7 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from optax->flax>=0.7.1->dm-haiku) (0.1.85)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from optax->flax>=0.7.1->dm-haiku) (0.4.23)\n",
      "Requirement already satisfied: etils[epath,epy] in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (1.6.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\alex_\\appdata\\roaming\\python\\python311\\site-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (1.5.6)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (4.23.4)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from chex>=0.1.7->optax->flax>=0.7.1->dm-haiku) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.1->dm-haiku) (0.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (2023.12.2)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (6.1.1)\n",
      "Requirement already satisfied: zipp in c:\\users\\alex_\\anaconda3\\envs\\jax_env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/deepmind/jraph.git\n",
    "!pip install flax\n",
    "!pip install dm-haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b307a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import jax.tree_util as tree\n",
    "import jraph\n",
    "import flax\n",
    "import haiku as hk\n",
    "import optax\n",
    "import pickle\n",
    "import numpy as onp\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from os import path\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from jax import random as jrandom\n",
    "from functools import partial\n",
    "\n",
    "data_loc = 'C:/Users/alex_/Fac/M2/M2/S2/Discrete_Graphs/Projet/Data/BlogCatalog3/BlogCatalog-dataset/data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f72e4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07063a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  [10312]\n",
      "Number of edges:  [333983]\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    iid = {}\n",
    "    idx = 0\n",
    "    sender_list = []  # List to store sender indices\n",
    "    receiver_list = []  # List to store receiver indices\n",
    "\n",
    "    # Read edges pairs\n",
    "    with open(data_loc+'edges.csv', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            i, j = line.strip().split(',')  # csv\n",
    "            if i not in iid:\n",
    "                iid[i] = idx; idx += 1\n",
    "            if j not in iid:\n",
    "                iid[j] = idx; idx += 1\n",
    "            sender_list.append(iid[i])  # Append sender index\n",
    "            receiver_list.append(iid[j])  # Append receiver index\n",
    "            \n",
    "            # Add the reverse edge as well\n",
    "            sender_list.append(iid[j])  # Append sender index\n",
    "            receiver_list.append(iid[i])  # Append receiver index\n",
    "\n",
    "    sender_array = jnp.array(sender_list)\n",
    "    receiver_array = jnp.array(receiver_list)\n",
    "    n_node = jnp.array([jnp.unique(sender_array).size])\n",
    "    n_edge = jnp.array([len(sender_list) // 2])\n",
    "\n",
    "    print(\"Number of nodes: \", n_node)\n",
    "    print(\"Number of edges: \", n_edge)\n",
    "\n",
    "    # Read labels\n",
    "    labels = jnp.zeros((n_node), dtype=int)\n",
    "    # Read (node_id, label) file\n",
    "    with open(data_loc+'group-edges.csv', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            node, group = line.strip().split(',') \n",
    "            labels = labels.at[iid[node]].set(int(group) - 1)\n",
    "\n",
    "    graph = jraph.GraphsTuple(\n",
    "      nodes=labels,\n",
    "      edges = None,\n",
    "      senders=sender_array,\n",
    "      receivers=receiver_array,\n",
    "      n_node=n_node,\n",
    "      n_edge=n_edge,\n",
    "      globals = None)\n",
    "    #bc_dataset = {'graph': graph, 'labels': labels}\n",
    "    return graph\n",
    "\n",
    "bc_dataset = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee578691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([333983], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_dataset.n_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746511e",
   "metadata": {},
   "source": [
    "## Visualizing the Graph\n",
    "To visualize the graph structure of the graph we created above, we will use the [`networkx`](networkx.org) library because it already has functions for drawing graphs.\n",
    "\n",
    "We first convert the `jraph.GraphsTuple` to a `networkx.DiGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921215e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
    "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
    "  nx_graph = nx.DiGraph()\n",
    "  if nodes is None:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n)\n",
    "  else:\n",
    "    for n in range(jraph_graph.n_node[0]):\n",
    "      nx_graph.add_node(n, node_feature=nodes[n])\n",
    "  if edges is None:\n",
    "    i = 0\n",
    "    print(jraph_graph.n_edge[0])\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      if i%10000 == 0:\n",
    "        print(i)\n",
    "      i +=1\n",
    "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
    "  else:\n",
    "    for e in range(jraph_graph.n_edge[0]):\n",
    "      nx_graph.add_edge(\n",
    "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
    "  return nx_graph\n",
    "\n",
    "\n",
    "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
    "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
    "  pos = nx.spring_layout(nx_graph)\n",
    "  nx.draw(\n",
    "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13deacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The graph is too big to be displayed\n",
    "#draw_jraph_graph_structure(bc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5baaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "38\n",
      "[[   0   60]\n",
      " [   1  488]\n",
      " [   2  365]\n",
      " [   3  119]\n",
      " [   4  625]\n",
      " [   5  563]\n",
      " [   6  393]\n",
      " [   7 1076]\n",
      " [   8  247]\n",
      " [   9  300]\n",
      " [  10  325]\n",
      " [  11   25]\n",
      " [  12   35]\n",
      " [  13  239]\n",
      " [  14   53]\n",
      " [  15  295]\n",
      " [  16  351]\n",
      " [  17  236]\n",
      " [  18  715]\n",
      " [  19  247]\n",
      " [  20  228]\n",
      " [  21  233]\n",
      " [  22  279]\n",
      " [  23  846]\n",
      " [  24  170]\n",
      " [  25  242]\n",
      " [  26   88]\n",
      " [  27   85]\n",
      " [  28  155]\n",
      " [  29  360]\n",
      " [  30   62]\n",
      " [  31  371]\n",
      " [  32   91]\n",
      " [  33   62]\n",
      " [  34   58]\n",
      " [  35  137]\n",
      " [  36   53]\n",
      " [  37   27]\n",
      " [  38    8]]\n"
     ]
    }
   ],
   "source": [
    "print(jnp.min(bc_dataset.nodes))\n",
    "print(jnp.max(bc_dataset.nodes))\n",
    "unique, counts = jnp.unique(bc_dataset.nodes, return_counts=True)\n",
    "print(jnp.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ae8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_walk_tensor(walk, graph , node, length, num_walks, rng_key):\n",
    "    walk = jnp.zeros((num_walks, length), dtype=jnp.int32)\n",
    "    walk = walk.at[:, 0].set(node)\n",
    "    j = 0\n",
    "    while j < num_walks:\n",
    "        current_node = node\n",
    "        step = 1\n",
    "        while step < length:\n",
    "            outgoing_edges = jnp.where(graph.senders == current_node)[0]\n",
    "            # Extraire les nœuds destinataires des arêtes sortantes\n",
    "            neighbors = jnp.take(graph.receivers, outgoing_edges)\n",
    "            rng_key, subkey = jrandom.split(rng_key)\n",
    "            current_node = jrandom.choice(subkey, neighbors)\n",
    "            walk = walk.at[j, step].set(current_node)\n",
    "            step += 1\n",
    "        j += 1\n",
    "    return walk, rng_key\n",
    "\n",
    "# Assert all edges exist\n",
    "num_walks = 5\n",
    "length = 5\n",
    "rng_key = jrandom.key(1)\n",
    "walk = jnp.zeros((num_walks, length), dtype=jnp.int32)\n",
    "\n",
    "rws, rng_key = gen_random_walk_tensor(walk,bc_dataset, 0, 5, 5,rng_key)\n",
    "for walk in range(5):\n",
    "    rw = rws[walk]\n",
    "    for step in range(4):\n",
    "        edge_exists = jnp.any(jnp.logical_and(bc_dataset.senders == rw[step], bc_dataset.receivers == rw[step+1]))\n",
    "        assert edge_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ec5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   91 1066 1298   40]\n",
      " [   0  101  363 3063 2255]\n",
      " [   0  103 1138  296  203]]\n"
     ]
    }
   ],
   "source": [
    "rws, rng_key = gen_random_walk_tensor(walk,bc_dataset, 0, 5, 3,rng_key)\n",
    "print(rws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3916250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    3  461 5583  246]\n",
      " [   0   43 4367    9 3677]\n",
      " [   0   80 3590    8 7455]\n",
      " [   1  161  617 3846  166]\n",
      " [   1 3115   25 3082   12]\n",
      " [   1 2172   76 4535 6754]]\n"
     ]
    }
   ],
   "source": [
    "def gen_batch_random_walk(batch_walk, graph, initial_nodes, length, num_walks, rng_key):\n",
    "    n_nodes = initial_nodes.shape[0]\n",
    "    walk = jnp.zeros((num_walks, length), dtype=jnp.int32)\n",
    "    batch_walk = jnp.zeros((num_walks * n_nodes, length), dtype=jnp.int32)\n",
    "    for i, n in enumerate(initial_nodes):\n",
    "        n = n.item()\n",
    "        rng_key, subkey = jrandom.split(rng_key)\n",
    "        sub_walk, rng_key = gen_random_walk_tensor(walk, graph, n, length, num_walks, subkey)\n",
    "        batch_walk = batch_walk.at[num_walks * i:num_walks * (i + 1)].set(sub_walk)\n",
    "    return walk, batch_walk , rng_key\n",
    "\n",
    "rng_key = jrandom.key(0)\n",
    "last_walk, batch_walk, rng_key = gen_batch_random_walk(walk, bc_dataset, jnp.array([0, 1]), 5, 3, rng_key)\n",
    "print(batch_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7eda3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5)\n",
      "(18, 3)\n"
     ]
    }
   ],
   "source": [
    "def generate_windows(windows, random_walk, window_size):\n",
    "    num_walks, walk_length = random_walk.shape\n",
    "    num_windows = walk_length + 1 - window_size\n",
    "    windows = jnp.zeros((num_walks * num_windows, window_size), dtype=jnp.int32)\n",
    "    for j in range(num_windows):\n",
    "        windows = windows.at[num_walks * j:num_walks * (j + 1)].set(random_walk[:, j:j + window_size])\n",
    "    return windows\n",
    "\n",
    "windows = jnp.zeros((1,1), dtype=int)\n",
    "windows = generate_windows(windows,batch_walk, 3)\n",
    "print(batch_walk.shape)\n",
    "print(windows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e0c029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  9.989289 ,  14.355413 ],\n",
       "       [  0.1050415,  -8.012434 ],\n",
       "       [  8.171717 ,  22.485273 ],\n",
       "       [-14.368268 ,  19.513103 ],\n",
       "       [ -4.4057875,  -8.634464 ],\n",
       "       [-10.255943 ,   3.4406242],\n",
       "       [-11.403227 , -12.996876 ],\n",
       "       [ -1.2675204,  17.343775 ],\n",
       "       [ -6.70873  ,  -7.203182 ],\n",
       "       [-19.548515 ,  23.787937 ],\n",
       "       [  8.799166 ,  29.117088 ],\n",
       "       [-23.854403 ,  -1.1407938],\n",
       "       [ 37.785007 , -10.838488 ],\n",
       "       [  9.711828 , -32.64926  ],\n",
       "       [ -3.9425344,   1.5056596],\n",
       "       [-11.72566  ,  11.133398 ],\n",
       "       [  5.2066326, -15.35693  ],\n",
       "       [ -1.85783  , -27.490053 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def get_windows_dotproduct(windows, embedding):\n",
    "    embedding_size = embedding.shape[1]\n",
    "    # get the embedding of the initial node repeated num_windows times\n",
    "    first_emb = embedding[windows[:, 0]]\n",
    "    first_emb = jnp.expand_dims(first_emb, axis=1)  # Ajouter une nouvelle dimension\n",
    "    # get the embedding of the remaining nodes in each window\n",
    "    others_emb = embedding[windows[:, 1:]]\n",
    "    others_emb = others_emb.reshape(windows.shape[0], -1, embedding_size)\n",
    "    # result has same shape as others\n",
    "    # Each element is the dot product between the corresponding node embedding\n",
    "    # and the embedding of the first node of that walk\n",
    "    # that is, result_{i, j} for random walk i and element j is v_{W_{i, 0}} dot v_{W_{i, j}}\n",
    "    result = jnp.sum(first_emb * others_emb, axis=-1)\n",
    "    return result\n",
    "\n",
    "embedding_jax = jax.random.normal(rng_key, shape=(12000, 300))\n",
    "get_windows_dotproduct(windows, embedding_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1cb5b8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-6.534585, dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def compute_mean_log_sigmoid(windows, embedding):\n",
    "    dot_product = get_windows_dotproduct(windows, embedding)\n",
    "    sigmoid_output = 1 / (1 + jnp.exp(-dot_product))  # Sigmoid function\n",
    "    log_sigmoid_output = jnp.log(sigmoid_output)  # Logarithm\n",
    "    return jnp.mean(log_sigmoid_output)  # Mean\n",
    "\n",
    "# Usage example:\n",
    "mean_log_sigmoid = compute_mean_log_sigmoid(windows, embedding_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c69545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[   0,  825,  911, 1631, 1314],\n",
       "        [   0,   28, 1399, 1362,  613],\n",
       "        [   0, 1806,  630, 1652,  360]], dtype=int32),\n",
       " Array((), dtype=key<fry>) overlaying:\n",
       " [3000548268 4272618543])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_negative_samples(amount, length, initial_node, number_of_nodes,rng_key):\n",
    "  \"\"\"Generates negative samples for a random walk process in JAX.\n",
    "\n",
    "  Args:\n",
    "    amount: Number of negative samples to generate.\n",
    "    length: Length of each negative sample walk (number of nodes in the path).\n",
    "    initial_node: Starting node for all negative samples.\n",
    "    number_of_nodes: Total number of nodes in the graph.\n",
    "\n",
    "  Returns:\n",
    "    A JAX array of shape (amount, length) containing the negative samples.\n",
    "  \"\"\"\n",
    "  negative_samples = jnp.zeros((amount, length), dtype=jnp.int32)  # Use jnp.int32 for node indices\n",
    "  negative_samples = negative_samples.at[:, 0].set(initial_node)  # Set initial node efficiently\n",
    "  rng_key, subkey = jrandom.split(rng_key)\n",
    "  negative_samples = negative_samples.at[:, 1:].set(\n",
    "      jrandom.randint(rng_key,shape=(amount, length - 1),minval = 0, maxval = number_of_nodes)\n",
    "  )\n",
    "  return negative_samples, rng_key\n",
    "\n",
    "gen_negative_samples(amount=3, length=5, initial_node=0, number_of_nodes=2000,rng_key =rng_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8fdede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[   0, 1180, 1940,  427,  957],\n",
       "        [   0, 1528, 1469,  905,   90],\n",
       "        [   0, 1610,  587,  474,  515],\n",
       "        [   1, 1402,   71, 1354, 1003],\n",
       "        [   1,  205, 1527, 1098,  918],\n",
       "        [   1,  859, 1883, 1862, 1594],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0]], dtype=int32),\n",
       " Array((), dtype=key<fry>) overlaying:\n",
       " [1887826715 4092242196])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_batch_negative_samples(amount, length, initial_nodes, number_of_nodes,rng_key):\n",
    "  \"\"\"Generates negative samples for a random walk process in JAX for a batch of initial nodes.\n",
    "\n",
    "  Args:\n",
    "    amount: Number of negative samples to generate per initial node.\n",
    "    length: Length of each negative sample walk (number of nodes in the path).\n",
    "    initial_nodes: A JAX array of shape (batch_size,) containing initial nodes for each sample.\n",
    "    number_of_nodes: Total number of nodes in the graph.\n",
    "\n",
    "  Returns:\n",
    "    A JAX array of shape (amount * batch_size, length) containing the negative samples.\n",
    "  \"\"\"\n",
    "\n",
    "  # Expand initial_nodes to match amount (amount, batch_size)\n",
    "  #initial_nodes = jnp.expand_dims(initial_nodes, axis=0).repeat(amount, axis=0)\n",
    "  n_nodes = initial_nodes.shape[0]\n",
    "  sub_negative_sample = jnp.zeros((num_walks, length), dtype=jnp.int32)\n",
    "  batch_negative_sample = jnp.zeros((num_walks * n_nodes, length), dtype=jnp.int32)\n",
    "  for i, n in enumerate(initial_nodes):\n",
    "      n = n.item()\n",
    "      rng_key, subkey = jrandom.split(rng_key)\n",
    "      sub_negative_sample, rng_key = gen_negative_samples(amount, length, n, number_of_nodes, subkey)\n",
    "      batch_negative_sample = batch_negative_sample.at[amount * i:amount * (i + 1)].set(sub_negative_sample)\n",
    "  return batch_negative_sample , rng_key\n",
    "\n",
    "\n",
    "# Example usage\n",
    "initial_nodes = jnp.array([0, 1])\n",
    "gen_batch_negative_samples(amount=3, length=5, initial_nodes=initial_nodes, number_of_nodes=2000,rng_key = rng_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d351cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[100]\n"
     ]
    }
   ],
   "source": [
    "def generate_batches(array, batch_size):\n",
    "    \"\"\"Yield successive batches of size `batch_size` from `array`.\"\"\"\n",
    "    for i in range(0, len(array), batch_size):\n",
    "        yield array[i:i + batch_size]\n",
    "\n",
    "gen = generate_batches(list(range(101)), 20)\n",
    "for batch in gen:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "eps = 1e-15\n",
    "\n",
    "def deepWalk(graph, walks_per_vertex, walk_length, window_size, embedding_size, num_neg, lr, epochs, batch_size):\n",
    "    number_of_nodes = jnp.array([jnp.unique(graph.senders).size])\n",
    "    \n",
    "    embedding = (torch.randn(size=(number_of_nodes, embedding_size)))\n",
    "    embedding = jax.random.normal(rng_key, shape=(number_of_nodes, embedding_size))\n",
    "    embedding.requires_grad = True\n",
    "    optimizer = torch.optim.SGD([embedding], lr=lr)\n",
    "    loss_history = {'pos': [], 'neg': [], 'total': []}\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        nodes = torch.tensor(list(graph.nodes), dtype=int)\n",
    "        random.shuffle(nodes)\n",
    "        node_loader = generate_batches(nodes, batch_size)\n",
    "        n_batches = int(number_of_nodes / batch_size)\n",
    "        for n in tqdm(node_loader, total=n_batches):\n",
    "            random_walk = gen_batch_random_walk(graph, n, walk_length, walks_per_vertex)\n",
    "            num_windows = walk_length + 1 - window_size\n",
    "\n",
    "            # Positive Sampling\n",
    "            # each row of windows is one window, we have B = walks_per_vertex*num_windows windows\n",
    "            windows = generate_windows(random_walk, window_size)\n",
    "            batch_dotproduct = get_windows_dotproduct(windows, embedding)\n",
    "            # takes the sigmoid of the dot product to get probability, then\n",
    "            # takes the loglik and average through all elements\n",
    "            pos_loss = -torch.log(torch.sigmoid(batch_dotproduct)+eps).mean()\n",
    "            # Negative Sampling\n",
    "            negative_samples = gen_batch_negative_samples(\n",
    "                amount=num_neg*walks_per_vertex, \n",
    "                length=walk_length, \n",
    "                initial_nodes=n, \n",
    "                number_of_nodes=number_of_nodes\n",
    "            )\n",
    "            windows = generate_windows(negative_samples, window_size)\n",
    "            batch_dotproduct = get_windows_dotproduct(windows, embedding)\n",
    "            neg_loss = -torch.log(1-torch.sigmoid(batch_dotproduct)+eps).mean()\n",
    "\n",
    "            loss = pos_loss + neg_loss\n",
    "            # Optimization\n",
    "            loss.backward()\n",
    "            loss_history['total'].append(loss.detach().numpy())\n",
    "            loss_history['pos'].append(pos_loss.detach().numpy())\n",
    "            loss_history['neg'].append(neg_loss.detach().numpy())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  \n",
    "\n",
    "    return embedding, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb3aa121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "\n",
    "eps = 1e-15\n",
    "\n",
    "### TO FIX\n",
    "\n",
    "def deepWalk(graph, walks_per_vertex, walk_length, window_size, embedding_size, num_neg, lr, epochs, batch_size,rng_key ,eps = 1e-15):\n",
    "    \n",
    "    number_of_nodes = int(jnp.unique(graph.senders).size)\n",
    "    \n",
    "    embedding = jax.random.normal(rng_key, shape=(number_of_nodes, embedding_size))\n",
    "    optimizer = optax.sgd(learning_rate=lr)\n",
    "    opt_state = optimizer.init(embedding)\n",
    "    loss_history = {'pos': [], 'neg': [], 'total': []}\n",
    "\n",
    "    #@jax.jit\n",
    "    def loss_fn(params, random_walk, neg_samples):\n",
    "        windows = 0\n",
    "        windows = generate_windows(windows, random_walk, window_size)\n",
    "        batch_dotproduct = get_windows_dotproduct(windows, params)\n",
    "        pos_loss = -jnp.log(jax.nn.sigmoid(batch_dotproduct) + eps).mean()\n",
    "\n",
    "        windows = generate_windows(windows,neg_samples, window_size)\n",
    "        batch_dotproduct = get_windows_dotproduct(windows, params)\n",
    "        neg_loss = -jnp.log(1 - jax.nn.sigmoid(batch_dotproduct) + eps).mean()\n",
    "\n",
    "        return pos_loss + neg_loss, pos_loss, neg_loss\n",
    "\n",
    "    #@jax.jit\n",
    "    def update(params, opt_state, random_walk, neg_samples):\n",
    "        print(\"voir\")\n",
    "        grad_fn = jax.value_and_grad(loss_fn)\n",
    "        print(\"essai\")\n",
    "        loss, grads = grad_fn(params, random_walk, neg_samples)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        nodes = jnp.array(list(graph.nodes), dtype=jnp.int32)\n",
    "        nodes = jax.random.permutation(rng_key, nodes)\n",
    "        node_loader = generate_batches(nodes, batch_size)\n",
    "        n_batches = int(number_of_nodes / batch_size)\n",
    "        batch_walk = 0\n",
    "        for n in tqdm(node_loader, total=n_batches):\n",
    "            walk, batch_walk , rng_key = gen_batch_random_walk(batch_walk, graph, n, walk_length, walks_per_vertex, rng_key)\n",
    "            batch_negative_sample , rng_key = gen_batch_negative_samples(\n",
    "                amount=num_neg * walks_per_vertex, \n",
    "                length=walk_length, \n",
    "                initial_nodes=n, \n",
    "                number_of_nodes=number_of_nodes,\n",
    "                rng_key = rng_key\n",
    "            )\n",
    "            print(loss_fn(embedding, batch_walk, batch_negative_sample))\n",
    "            embedding, opt_state, loss = update(embedding, opt_state, batch_walk, batch_negative_sample)\n",
    "            print(\"test\")\n",
    "            loss_history['total'].append(loss[0])\n",
    "            loss_history['pos'].append(loss[1])\n",
    "            loss_history['neg'].append(loss[2])\n",
    "\n",
    "    return embedding, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9036e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6d17aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array(nan, dtype=float32), Array(nan, dtype=float32), Array(nan, dtype=float32))\n",
      "voir\n",
      "essai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Gradient only defined for scalar-output functions. Output was (Array(nan, dtype=float32), Array(nan, dtype=float32), Array(nan, dtype=float32)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alex_\\anaconda3\\envs\\jax_env\\Lib\\site-packages\\jax\\_src\\core.py:1498\u001b[0m, in \u001b[0;36mget_aval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1498\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alex_\\anaconda3\\envs\\jax_env\\Lib\\site-packages\\jax\\_src\\core.py:1490\u001b[0m, in \u001b[0;36mconcrete_aval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1489\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_aval(x\u001b[38;5;241m.\u001b[39m__jax_array__())\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1491\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Value (Array(nan, dtype=float32), Array(nan, dtype=float32), Array(nan, dtype=float32)) with type <class 'tuple'> is not a valid JAX type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding, loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mdeepWalk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbc_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwalks_per_vertex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwalk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrng_key\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 57\u001b[0m, in \u001b[0;36mdeepWalk\u001b[1;34m(graph, walks_per_vertex, walk_length, window_size, embedding_size, num_neg, lr, epochs, batch_size, rng_key, eps)\u001b[0m\n\u001b[0;32m     49\u001b[0m batch_negative_sample , rng_key \u001b[38;5;241m=\u001b[39m gen_batch_negative_samples(\n\u001b[0;32m     50\u001b[0m     amount\u001b[38;5;241m=\u001b[39mnum_neg \u001b[38;5;241m*\u001b[39m walks_per_vertex, \n\u001b[0;32m     51\u001b[0m     length\u001b[38;5;241m=\u001b[39mwalk_length, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     rng_key \u001b[38;5;241m=\u001b[39m rng_key\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_fn(embedding, batch_walk, batch_negative_sample))\n\u001b[1;32m---> 57\u001b[0m embedding, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_walk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_negative_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m loss_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[70], line 36\u001b[0m, in \u001b[0;36mdeepWalk.<locals>.update\u001b[1;34m(params, opt_state, random_walk, neg_samples)\u001b[0m\n\u001b[0;32m     34\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_walk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n\u001b[0;32m     38\u001b[0m new_params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\alex_\\anaconda3\\envs\\jax_env\\Lib\\site-packages\\jax\\_src\\api.py:755\u001b[0m, in \u001b[0;36m_check_scalar\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    753\u001b[0m   aval \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mget_aval(x)\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 755\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(aval, ShapedArray):\n",
      "\u001b[1;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output was (Array(nan, dtype=float32), Array(nan, dtype=float32), Array(nan, dtype=float32))."
     ]
    }
   ],
   "source": [
    "embedding, loss_history = deepWalk(\n",
    "    graph=bc_dataset,  \n",
    "    walks_per_vertex=1, \n",
    "    walk_length=2, \n",
    "    window_size=1,  \n",
    "    embedding_size=128,\n",
    "    num_neg=2,\n",
    "    lr=1,\n",
    "    epochs=1,\n",
    "    batch_size=256,\n",
    "    rng_key = rng_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_vec = np.cumsum(np.insert(loss_history['total'], 0, 0)) \n",
    "window_width = 10\n",
    "ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "plt.plot(ma_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X-X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2255e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = embedding.detach().numpy()\n",
    "y = bc_dataset['labels']\n",
    "\n",
    "clf = LogisticRegression(random_state=0, multi_class='ovr').fit(X_norm, y)\n",
    "y_hat = clf.predict(X_norm)\n",
    "f1_score(y, y_hat, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cffcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_t = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_t[:, 0], X_t[:, 1], c=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
